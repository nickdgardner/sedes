#!/usr/bin/env python3

import getopt
import re
import sys

from bs4 import BeautifulSoup

import hexameter.scan

TLG_MAP = {
    "0001:001": "Ap.Rh.",
    "0005:001": "Theoc.",
    "0012:001": "Iliad",
    "0012:002": "Ody.",
    "0013:002": "Hymns", # "Hymni Homerici, In Cererem"
    "0013:003": "Hymns", # "Hymni Homerici, In Apollinem"
    "0013:004": "Hymns", # "Hymni Homerici, In Mercurium"
    "0013:005": "Hymns", # "Hymni Homerici, In Venerem"
    "0020:001": "Theog.",
    "0020:002": "WD",
    "0020:003": "Shield",
    "0020:004": "Fr.", # ??? "Fragmenta"
    "0020:006": "Fr.", # ??? "Fragmenta astronomica"
    "0653:001": "Arat",
}

# Apply various cleanup functions to the document tree for easier parsing.
def fixup(soup):
    # Unwrap spacer elements. The tags are not closed in the input, so they
    # technically introduce an extra level of hierarchy wherever they are used.
    for elem in soup.find_all("spacer"):
        elem.unwrap()
    # Similarly unwrap <font size="..."></font> elements. The size="-1" ones are
    # not closed (and also seem to have the side effect of capitalizing certain
    # strings within them, e.g. "parse_lat"->"PARSE_LAT" and
    # "jumpTo('tlg...')"â†’"jumpTo('TLG...')").
    for elem in soup.find_all("font", size=True):
        elem.unwrap()
    # Get rid of empty anchors.
    for elem in soup.find_all("a"):
        if not elem.contents:
            elem.decompose()
    # Get rid of basefont spam.
    for elem in soup.find_all("basefont"):
        elem.decompose()
    # Replace Windows-1252 quotation marks.
    strings = []
    for string in soup.strings:
        strings.append(string)
    for string in strings:
        string.replace_with(str(string).replace(u"\u0093", u"\u201c").replace(u"\u0094", u"\u201d"))

# The results are flat siblings separated by hr elements. Take the pieces
# between each pair of hr elements (including notional ones at the beginning and
# end) and wrap them in section elements.
def split_sections(soup, container):
    section = soup.new_tag("section")
    for elem in list(container.contents):
        elem = elem.extract()
        if elem.name == "hr":
            container.append(section)
            section = soup.new_tag("section")
        else:
            section.append(elem)
    container.append(section)

def parse_jumpto(s):
    # Extract work ID and line number from a Diogenese HTML jumpTo reference, as
    # found in the onlick attribute of links. The links look like:
    #   <a onclick="jumpTo('tlg,0001,001:2:394');">
    #
    # "tlg" is normally lowercase, but it was once uppercase in this context:
    # <a onclick="parse_lat('Georg')">Georg.</a>
    # <font size="-1">
    #   <a onclick="PARSE_LAT('I')">I.</a>
    #   <br>
    #   <p>
    #     <a onclick="jumpTo('TLG,0020,004:291:4');">Go to Context</a>
    #   </p>
    #   <hr>
    # </font>
    m = re.match(r'^jumpTo\(\'(?:tlg|TLG),(\w+),(\w+):?([\w:]+)\'\);$', s)
    assert m is not None
    if m.group(1) == "0013":
        # 0013 is a special case for the Homeric Hymns; the second number
        # indicates which hymn it is. We still want TLG_MAP to map the first two
        # numbers to the string "Hymns", but we want to give an indication of
        # which hymn along with the line number.
        return m.group(1) + ":" + m.group(2), m.group(2).lstrip("0") + ":" + m.group(3)
    else:
        return m.group(1) + ":" + m.group(2), m.group(3)

# Return a 2-tuple of "Incidence of all words as reported by word list" and
# "Passages containing those words reported by Diogenes".
def get_counts(elem):
    m = re.search(r'\bIncidence of all words as reported by word list: (\d+)\nPassages containing those words reported by Diogenes: (\d+)\n', elem.get_text())
    return int(m.group(1)), int(m.group(2))

def find_metrical_position(scansion, offset):
    # word = []
    # for c, _, s in scansion:
    #     if " " in c:
    #         word.append(c)
    #     elif s != "|":
    #         word.append(s)
    # print(scansion)
    # print("".join(word), offset)
    pos = 0
    for c, _, s in scansion:
        if offset == 0:
            break
        if " " in c:
            offset -= 1
        elif s == "+":
            pos += 1
        elif s == "-":
            pos += 0.5
    return pos + 1

def process_section(section):
    marked_offset = None
    marked_form = None
    marked_line = None
    saw_marked = False
    line = []
    offset = 0
    # Walk each element, treating <br> as the end of a line. Note when we've
    # seen a word with <font color="red">.
    for elem in section.children:
        if elem.name == "br":
            if saw_marked:
                marked_line = u"".join(line).strip()
            saw_marked = False
            line = []
            offset = 0
        elif elem.name == "a":
            if elem.find("font", color="red"):
                saw_marked = True
                marked_offset = offset
                marked_form = elem.get_text()
            offset += 1
        try:
            line.append(elem.get_text())
        except AttributeError:
            line.append(str(elem))

    context_link = section.find("p", string="Go to Context", recursive=False)
    assert context_link is not None, "no context found in %s" % section
    work, pos = parse_jumpto(context_link.a["onclick"])

    # Some lines start with '>'; remove it.
    marked_line = re.sub(r'^\s*>\s*', "", marked_line)

    metrical_positions = []
    scansions = hexameter.scan.analyze_line_metrical(marked_line)
    if not scansions:
        # Unable to scan.
        metrical_positions.append("?")
    for scansion in scansions:
        metrical_positions.append("%g" % find_metrical_position(scansion, marked_offset))

    print(TLG_MAP.get(work, work), pos, "/".join(metrical_positions), marked_form, marked_line, sep='\t')

def process(soup):
    fixup(soup)

    main_window = soup.find(id="main_window")
    split_sections(soup, main_window)
    sections = main_window.find_all("section")

    # First section is a header/summary.
    # Penultimate section is a count of incidences.
    # Final section is a footer.
    for section in sections[1:-2]:
        process_section(section)

    incidence_count, passage_count = get_counts(sections[-2])
    assert passage_count == len(sections)-3, "expected %d passages, found %d" % (passage_count, len(sections)-3)

_, args = getopt.gnu_getopt(sys.argv[1:], "")
if args:
    for filename in args:
        with open(filename) as f:
            process(BeautifulSoup(f, "lxml"))
else:
    process(BeautifulSoup(sys.stdin, "lxml"))
