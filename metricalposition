#!/usr/bin/env python3

import getopt
import re
import sys

from bs4 import BeautifulSoup

import hexameter.scan

TLG_MAP = {
    "0001:001": "Ap.Rh.",
    "0005:001": "Theoc.",
    "0012:001": "Iliad",
    "0012:002": "Ody.",
    "0013:001": "Hymns", # "Hymni Homerici, in Bacchum"
    "0013:002": "Hymns", # "Hymni Homerici, In Cererem"
    "0013:003": "Hymns", # "Hymni Homerici, In Apollinem"
    "0013:004": "Hymns", # "Hymni Homerici, In Mercurium"
    "0013:005": "Hymns", # "Hymni Homerici, In Venerem"
    "0013:006": "Hymns", # "Hymni Homerici, In Venerem"
    "0013:007": "Hymns", # "Hymni Homerici, In Bacchum"
    "0013:008": "Hymns", # "Hymni Homerici, In Martem"
    "0013:009": "Hymns", # "Hymni Homerici, In Dianam"
    "0013:010": "Hymns", # "Hymni Homerici, In Venerem"
    "0013:011": "Hymns", # "Hymni Homerici, In Minervam"
    "0013:012": "Hymns", # "Hymni Homerici, In Junonem"
    "0013:013": "Hymns", # "Hymni Homerici, In Cererem"
    "0013:014": "Hymns", # "Hymni Homerici, In matrem deorum"
    "0013:015": "Hymns", # "Hymni Homerici, In Herculem"
    "0013:016": "Hymns", # "Hymni Homerici, In Aesculapium"
    "0013:017": "Hymns", # "Hymni Homerici, In Dioscuros"
    "0013:018": "Hymns", # "Hymni Homerici, In Mercurium"
    "0013:019": "Hymns", # "Hymni Homerici, In Pana"
    "0013:020": "Hymns", # "Hymni Homerici, In Volcanum"
    "0013:021": "Hymns", # "Hymni Homerici, In Apollinem"
    "0013:022": "Hymns", # "Hymni Homerici, In Neptunum"
    "0013:023": "Hymns", # "Hymni Homerici, In Jovem"
    "0013:024": "Hymns", # "Hymni Homerici, In Vestam"
    "0013:025": "Hymns", # "Hymni Homerici, In Musas et Apollinem"
    "0013:026": "Hymns", # "Hymni Homerici, In Bacchum"
    "0013:027": "Hymns", # "Hymni Homerici, In Dianam"
    "0013:028": "Hymns", # "Hymni Homerici, In Minervam"
    "0013:029": "Hymns", # "Hymni Homerici, In Vestam"
    "0013:030": "Hymns", # "Hymni Homerici, In Tellurem matrem omnium"
    "0013:031": "Hymns", # "Hymni Homerici, In Solem"
    "0013:032": "Hymns", # "Hymni Homerici, In Lunam"
    "0013:033": "Hymns", # "Hymni Homerici, In Dioscuros"
    "0013:034": "Hymns", # "Hymni Homerici, Εἰς ξένους"
    "0020:001": "Theog.",
    "0020:002": "WD",
    "0020:003": "Shield",
    "0020:004": "Fr.", # ??? "Fragmenta"
    "0020:005": "Fr.", # ??? "Testimonia"
    "0020:006": "Fr.", # ??? "Fragmenta astronomica"
    "0020:007": "Fr.", # ??? "Fragmenta"
    "0533:009": "Call.", # "Hecala"
    "0533:015": "Call.", # "In Jovem"
    "0533:016": "Call.", # "In Apollinem"
    "0533:017": "Call.", # "In Dianam"
    "0533:018": "Call.", # "In Delum"
    "0533:020": "Call.", # "In Cererem"
    "0653:001": "Arat",
}

# This is a hack to avoid duplicate erroneous output lines when Diogenes outputs
# overlapping fragments. We keep track of the refs we have already output, and
# ignore an entire fragment whenever we supposedly come across one we've already
# seen.
SEEN = set()

# Represents a TLG location reference.
class Ref(object):
    def __init__(self):
        self.author_id = None
        self.work_id = None
        # A list of more specific locator references, e.g. [], [book], or [book, epigram].
        self.refs = None
        self.lineno = None
        # lineno_junk is annotations that may come after the line number, such
        # as an asterisk.
        self.lineno_junk = ""

    def __str__(self):
        return self.author_id + "," + ":".join([self.work_id] + self.refs + [str(self.lineno) + self.lineno_junk])

    def key(self):
        return tuple([self.author_id, self.work_id] + self.refs + [self.lineno, self.lineno_junk])

    def author_work(self):
        return self.author_id + ":" + self.work_id

    def location(self):
        refs = self.refs
        if self.author_id == "0013":
            # author_id==0013 is a special case for the Homeric Hymns; the
            # work_id indicates which hymn it is.
            refs = [self.work_id.lstrip("0")]
        return ":".join(refs + [str(self.lineno) + self.lineno_junk])

# Apply various cleanup functions to the document tree for easier parsing.
def fixup(soup):
    # Unwrap spacer elements. The tags are not closed in the input, so they
    # technically introduce an extra level of hierarchy wherever they are used.
    for elem in soup.find_all("spacer"):
        elem.unwrap()
    # Similarly unwrap <font size="..."></font> elements. The size="-1" ones are
    # not closed (and also seem to have the side effect of capitalizing certain
    # strings within them, e.g. "parse_lat"->"PARSE_LAT" and
    # "jumpTo('tlg...')"→"jumpTo('TLG...')").
    for elem in soup.find_all("font", size=True):
        elem.unwrap()
    # Get rid of empty anchors.
    for elem in soup.find_all("a"):
        if not elem.contents:
            elem.decompose()
    # Get rid of basefont spam.
    for elem in soup.find_all("basefont"):
        elem.decompose()

# The results are flat siblings separated by hr elements. Take the pieces
# between each pair of hr elements (including notional ones at the beginning and
# end) and wrap them in section elements.
def split_sections(soup, container):
    section = soup.new_tag("section")
    for elem in list(container.contents):
        elem = elem.extract()
        if elem.name == "hr":
            container.append(section)
            section = soup.new_tag("section")
        else:
            section.append(elem)
    container.append(section)

# Get rid of <p><b></b></p> markup of headings.
#
# For example,
# <p><b>
#   <a onclick="parse_grk('%2Ae%2Ar%2Ag%2Aa')"><font color="red"><b><u>ΕΡΓΑ</u></b></font></a>
#   <a onclick="parse_grk('%2Ak%2Aa%2Ai')">ΚΑΙ</a>
#   <a onclick="parse_grk('%2Ah%2Am%2Ae%2Ar%2Aa%2Ai')">ΗΜΕΡΑΙ</a></font>
# </b></p>
def unwrap_container(container):
    changed = True
    while changed:
        changed = False
        for elem in container.children:
            if elem.name in ("p", "b"):
                elem.unwrap()
                changed = True

# Return elements between br elements as a list of span elements. A trailing br
# is ignored, as if br were a '\n' line terminator.
def split_lines(soup, container):
    result = []
    span = soup.new_tag("span")
    unwrap_container(container)
    for elem in list(container.contents):
        if elem.name == "br":
            result.append(span)
            span = soup.new_tag("span")
        else:
            span.append(elem)
    if span.get_text().strip():
        result.append(span)
    return result

def parse_jumpto(s):
    # Extract a Ref from a Diogenese HTML jumpTo reference, as found in the
    # onlick attribute of links. The links have the form:
    #   <a onclick="jumpTo('tlg,[author],[work]:[book]:[epigram]:[lineno]');">
    # or
    #   <a onclick="jumpTo('tlg,[author],[work]:[book]:[lineno]');">
    # or
    #   <a onclick="jumpTo('tlg,[author],[work]:[lineno]');">
    # for example,
    #   <a onclick="jumpTo('tlg,0012,003:7:153:1');">
    #   <a onclick="jumpTo('tlg,0001,001:2:394');">
    #   <a onclick="jumpTo('tlg,0020,002:89');">
    # There may be extra junk after the line number:
    #   <a onclick="jumpTo('tlg,0001,001:1:1250*');">
    #
    # In general, we stick everything between [work] and [lineno] into the
    # Ref.refs field.
    #
    # "tlg" is normally lowercase, but it can be uppercase, for example in this
    # context:
    # <a onclick="parse_lat('Georg')">Georg.</a>
    # <font size="-1">
    #   <a onclick="PARSE_LAT('I')">I.</a>
    #   <br>
    #   <p>
    #     <a onclick="jumpTo('TLG,0020,004:291:4');">Go to Context</a>
    #   </p>
    #   <hr>
    # </font>
    m = re.match(r'^jumpTo\(\'tlg,(\w+),([\w():*]+)\'\);$', s, flags=re.I)
    assert m is not None, repr(s)

    author_id = m.group(1)
    parts = m.group(2).split(":")
    work_id, refs, lineno = parts[0], parts[1:-1], parts[-1]
    m = re.match(r'^(t|[0-9]+)(.*)$', lineno)
    if m is None:
        raise ValueError("cannot parse line number %r" % lineno)
    lineno = m.group(1)
    if lineno == "t":
        lineno = 0
    else:
        lineno = int(lineno)
    lineno_junk = m.group(2)

    ref = Ref()
    ref.author_id = author_id
    ref.work_id = work_id
    ref.refs = refs
    ref.lineno = lineno
    ref.lineno_junk = lineno_junk
    return ref

# Return a 2-tuple of "Incidence of all words as reported by word list" and
# "Passages containing those words reported by Diogenes". Raises ValueError in
# case of error.
def get_counts(elem):
    m = re.search(r'\bIncidence of all words as reported by word list: (\d+)\nPassages containing those words reported by Diogenes: (\d+)\n', elem.get_text())
    if m is None:
        raise ValueError("no summary found")
    return int(m.group(1)), int(m.group(2))

def find_metrical_position(scansion, offset):
    words = []
    word = []
    for c, _, s in scansion:
        if " " in c:
            words.append("".join(word))
            word = []
        elif s == "+" or s == "-":
            word.append(s)
    if word:
        words.append("".join(word))

    pos = 0
    for i in range(0, offset):
        for s in words[i]:
            if s == "+":
                pos += 1
            elif s == "-":
                pos += 0.5
    # Now consume one more syllable. We seek ahead to the first scansion element
    # that is not '', because some words (like "δ'") have no metrical value on
    # their own, but inherit that of the following word.
    i = offset
    while not words[i]:
        i += 1
    if words[i][0] == "+":
        pos += 1
    elif words[i][0] == "-":
        pos += 0.5
    else:
        assert False

    return pos

def unique(a):
    result = []
    for x in a:
        if x not in result:
            result.append(x)
    return result

def fixup_line(line):
    # Some lines start with weird symbols; remove them.
    return re.sub(r'^\s*[>—⸖※]*\s*', "", line)

def process_word(ref, line, form, offset):
    if ref.lineno == 0:
        # A lineno of 0 indicates a title heading.
        return

    metrical_positions = []
    scansions = hexameter.scan.analyze_line_metrical(fixup_line(line))
    if not scansions:
        # Unable to scan.
        metrical_positions.append("?")
    for scansion in scansions:
        metrical_positions.append("%g" % find_metrical_position(scansion, offset))
    metrical_positions = unique(metrical_positions)

    work_str = ref.author_work()
    print(TLG_MAP.get(work_str, work_str), ref.location(), "/".join(metrical_positions), form, line, sep='\t')

def process_section(soup, section):
    context_link = section.find("p", string="Go to Context", recursive=False)
    if context_link is None:
        raise ValueError("can't find context link: " + str(section))
    context_link = context_link.extract()
    ref = parse_jumpto(context_link.a["onclick"])

    if ref.key() in SEEN:
        print("%s already seen; ignoring this section" % ref, file=sys.stderr)
        return

    # lines will contain also the section header (with the author, title, etc.).
    # It's okay because we only really start paying attention once we see the
    # first highlighted word, which can only appear inside the lines of the
    # actual quotation.
    lines = split_lines(soup, section)

    # Skip lines until we find the first one with a highlighted word; that's the
    # one the ref refers to.
    for i in range(len(lines)):
        line = lines[i]
        if line.find("font", color="red"):
            break
    else:
        raise ValueError("no highlighted words in %s" % ref)

    for i in range(i, len(lines)):
        line = lines[i]
        text = line.get_text().strip()
        # Output all the highlighted words on this line.
        offset = 0
        for elem in line.children:
            if elem.name == "a":
                if elem.find("font", color="red"):
                    process_word(ref, text, elem.get_text(), offset)
                    SEEN.add(ref.key())
                offset += 1
        # There may be further highlighted words on following lines; increment
        # the line number.
        ref.lineno += 1
        ref.lineno_junk = ""

def process(f):
    data = f.read()
    # Diogenes HTML output contains a lot of unclosed spacer elements. They each
    # introduce an extra layer of hierarchy and so the fixup function unwraps
    # all of them. But the unwrap operation is slow because the elements are so
    # numerous and deep. Here we do a prefilter to erase them before converting
    # the document to soup.
    data = re.sub(r'<spacer type="horizontal" size="\d+">', "", data)

    soup = BeautifulSoup(data, "lxml")

    fixup(soup)

    main_window = soup.find(id="main_window")
    split_sections(soup, main_window)
    sections = main_window.find_all("section")
    # First section is a header/summary.
    # Penultimate section is a count of incidences.
    # Final section is a footer.
    try:
        incidence_count, passage_count = get_counts(sections[-2])
        sections = sections[1:-2]
    except ValueError:
        # But some files lack the footer, and instead have a final empty
        # section.
        incidence_count, passage_count = None, None
        assert sections[-1].get_text().strip() == ""
        sections = sections[1:-1]

    for section in sections:
        process_section(soup, section)

    # assert incidence_count is None or incidence_count == len(SEEN), "expected %d incidences, found %d" % (incidence_count, len(SEEN))
    assert passage_count is None or passage_count == len(sections), "expected %d passages, found %d" % (passage_count, len(sections))

_, args = getopt.gnu_getopt(sys.argv[1:], "")
if args:
    for filename in args:
        with open(filename) as f:
            process(f)
else:
    process(sys.stdin)
