#!/usr/bin/env python3

import getopt
import re
import sys

from bs4 import BeautifulSoup

import hexameter.scan

TLG_MAP = {
    "0001:001": "Ap.Rh.",
    "0005:001": "Theoc.",
    "0012:001": "Iliad",
    "0012:002": "Ody.",
    "0013:002": "Hymns", # "Hymni Homerici, In Cererem"
    "0013:003": "Hymns", # "Hymni Homerici, In Apollinem"
    "0013:004": "Hymns", # "Hymni Homerici, In Mercurium"
    "0013:005": "Hymns", # "Hymni Homerici, In Venerem"
    "0020:001": "Theog.",
    "0020:002": "WD",
    "0020:003": "Shield",
    "0020:004": "Fr.", # ??? "Fragmenta"
    "0020:006": "Fr.", # ??? "Fragmenta astronomica"
    "0653:001": "Arat",
}

# This is a hack to avoid duplicate erroneous output lines when Diogenes outputs
# overlapping fragments. We keep track of the refs we have already output, and
# ignore an entire fragment whenever we supposedly come across one we've already
# seen.
SEEN = set()

# Represents a TLG location reference.
class Ref(object):
    def __init__(self):
        self.author_id = None
        self.work_id = None
        self.book_id = None
        self.lineno = None

    def __str__(self):
        s = self.author_id + ","+ self.work_id
        if self.book_id is not None:
            s += ":" + self.book_id
        s += ":" + str(self.lineno)
        return s

    def key(self):
        return (self.author_id, self.work_id, self.book_id, self.lineno)

    def author_work(self):
        return self.author_id + ":" + self.work_id

    def location(self):
        book_id = self.book_id
        if self.author_id == "0013":
            # author_id==0013 is a special case for the Homeric Hymns; the work_id
            # indicates which hymn it is. We assign the work_id to the book_id as
            # well, for the sake of having the humn number next to each line number.
            assert book_id is None
            book_id = self.work_id
        if book_id is not None:
            return book_id.lstrip("0") + ":" + str(self.lineno)
        else:
            return str(self.lineno)

# Apply various cleanup functions to the document tree for easier parsing.
def fixup(soup):
    # Unwrap spacer elements. The tags are not closed in the input, so they
    # technically introduce an extra level of hierarchy wherever they are used.
    for elem in soup.find_all("spacer"):
        elem.unwrap()
    # Similarly unwrap <font size="..."></font> elements. The size="-1" ones are
    # not closed (and also seem to have the side effect of capitalizing certain
    # strings within them, e.g. "parse_lat"->"PARSE_LAT" and
    # "jumpTo('tlg...')"→"jumpTo('TLG...')").
    for elem in soup.find_all("font", size=True):
        elem.unwrap()
    # Get rid of empty anchors.
    for elem in soup.find_all("a"):
        if not elem.contents:
            elem.decompose()
    # Get rid of basefont spam.
    for elem in soup.find_all("basefont"):
        elem.decompose()
    # Replace Windows-1252 quotation marks.
    strings = []
    for string in soup.strings:
        strings.append(string)
    for string in strings:
        string.replace_with(str(string).replace(u"\u0093", u"\u201c").replace(u"\u0094", u"\u201d"))

# The results are flat siblings separated by hr elements. Take the pieces
# between each pair of hr elements (including notional ones at the beginning and
# end) and wrap them in section elements.
def split_sections(soup, container):
    section = soup.new_tag("section")
    for elem in list(container.contents):
        elem = elem.extract()
        if elem.name == "hr":
            container.append(section)
            section = soup.new_tag("section")
        else:
            section.append(elem)
    container.append(section)

# Return elements between br elements as a list of span elements. A trailing br
# is ignored, as if br were a '\n' line terminator.
def split_lines(soup, container):
    result = []
    span = soup.new_tag("span")
    for elem in list(container.contents):
        if elem.name == "br":
            result.append(span)
            span = soup.new_tag("span")
        else:
            span.append(elem)
    if span.get_text().strip():
        result.append(span)
    return result

def parse_jumpto(s):
    # Extract a Ref from a Diogenese HTML jumpTo reference, as found in the
    # onlick attribute of links. The links have the form:
    #   <a onclick="jumpTo('tlg,[author],[work]:[book]:[lineno]');">
    # or
    #   <a onclick="jumpTo('tlg,[author],[work]:[lineno]');">
    # for example,
    #   <a onclick="jumpTo('tlg,0001,001:2:394');">
    #   <a onclick="jumpTo('tlg,0020,002:89');">
    #
    # "tlg" is normally lowercase, but it can be uppercase, for example in this
    # context:
    # <a onclick="parse_lat('Georg')">Georg.</a>
    # <font size="-1">
    #   <a onclick="PARSE_LAT('I')">I.</a>
    #   <br>
    #   <p>
    #     <a onclick="jumpTo('TLG,0020,004:291:4');">Go to Context</a>
    #   </p>
    #   <hr>
    # </font>
    m = re.match(r'^jumpTo\(\'tlg,(\w+),([\w():]+)\'\);$', s, flags=re.I)
    assert m is not None, repr(s)

    author_id = m.group(1)
    parts = m.group(2).split(":")
    try:
        work_id, book_id, lineno = parts
    except ValueError:
        work_id, lineno = parts
        book_id = None

    ref = Ref()
    ref.author_id = author_id
    ref.work_id = work_id
    ref.book_id = book_id
    ref.lineno = int(lineno)
    return ref

# Return a 2-tuple of "Incidence of all words as reported by word list" and
# "Passages containing those words reported by Diogenes".
def get_counts(elem):
    m = re.search(r'\bIncidence of all words as reported by word list: (\d+)\nPassages containing those words reported by Diogenes: (\d+)\n', elem.get_text())
    return int(m.group(1)), int(m.group(2))

def find_metrical_position(scansion, offset):
    # word = []
    # for c, _, s in scansion:
    #     if " " in c:
    #         word.append(c)
    #     elif s != "|":
    #         word.append(s)
    # print(scansion)
    # print("".join(word), offset)
    pos = 0
    for c, _, s in scansion:
        if offset == 0:
            break
        if " " in c:
            offset -= 1
        elif s == "+":
            pos += 1
        elif s == "-":
            pos += 0.5
    return pos + 1

def unique(a):
    result = []
    for x in a:
        if x not in result:
            result.append(x)
    return result

def fixup_line(line):
    # Some lines start with weird symbols; remove them.
    return re.sub(r'^\s*[>—⸖※]\s*', "", line)

def process_word(ref, line, form, offset):
    metrical_positions = []
    scansions = hexameter.scan.analyze_line_metrical(fixup_line(line))
    if not scansions:
        # Unable to scan.
        metrical_positions.append("?")
    for scansion in scansions:
        metrical_positions.append("%g" % find_metrical_position(scansion, offset))
    metrical_positions = unique(metrical_positions)

    work_str = ref.author_work()
    print(TLG_MAP.get(work_str, work_str), ref.location(), "/".join(metrical_positions), form, line, sep='\t')

def process_section(soup, section):
    context_link = section.find("p", string="Go to Context", recursive=False)
    context_link = context_link.extract()
    ref = parse_jumpto(context_link.a["onclick"])

    if ref.key() in SEEN:
        print("%s already seen; ignoring this section" % ref, file=sys.stderr)
        return

    # lines will contain also the section header (with the author, title, etc.).
    # It's okay because we only really start paying attention once we see the
    # first highlighted word, which can only appear inside the lines of the
    # actual quotation.
    lines = split_lines(soup, section)

    # Skip lines until we find the first one with a highlighted word; that's the
    # one the ref refers to.
    for i in range(len(lines)):
        line = lines[i]
        if line.find("font", color="red"):
            break
    else:
        raise ValueError("no highlighted words in %s" % ref)

    for i in range(i, len(lines)):
        line = lines[i]
        text = line.get_text().strip()
        # Output all the highlighted words on this line.
        offset = 0
        for elem in line.children:
            if elem.name == "a":
                if elem.find("font", color="red"):
                    process_word(ref, text, elem.get_text(), offset)
                    SEEN.add(ref.key())
                offset += 1
        # There may be further highlighted words on following lines; increment
        # the line number.
        ref.lineno += 1

def process(soup):
    fixup(soup)

    main_window = soup.find(id="main_window")
    split_sections(soup, main_window)
    sections = main_window.find_all("section")
    # First section is a header/summary.
    # Penultimate section is a count of incidences.
    # Final section is a footer.
    incidence_count, passage_count = get_counts(sections[-2])
    sections = sections[1:-2]

    for section in sections:
        process_section(soup, section)

    assert passage_count == len(sections), "expected %d passages, found %d" % (passage_count, len(sections))

_, args = getopt.gnu_getopt(sys.argv[1:], "")
if args:
    for filename in args:
        with open(filename) as f:
            process(BeautifulSoup(f, "lxml"))
else:
    process(BeautifulSoup(sys.stdin, "lxml"))
