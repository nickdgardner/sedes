#!/usr/bin/env python3

import getopt
import re
import sys

from bs4 import BeautifulSoup

import hexameter.scan

TLG_MAP = {
    "0001:001": "Ap.Rh.",
    "0005:001": "Theoc.",
    "0012:001": "Iliad",
    "0012:002": "Ody.",
    "0013:001": "Hymns", # "Hymni Homerici, in Bacchum"
    "0013:002": "Hymns", # "Hymni Homerici, In Cererem"
    "0013:003": "Hymns", # "Hymni Homerici, In Apollinem"
    "0013:004": "Hymns", # "Hymni Homerici, In Mercurium"
    "0013:005": "Hymns", # "Hymni Homerici, In Venerem"
    "0013:006": "Hymns", # "Hymni Homerici, In Venerem"
    "0013:007": "Hymns", # "Hymni Homerici, In Bacchum"
    "0013:008": "Hymns", # "Hymni Homerici, In Martem"
    "0013:009": "Hymns", # "Hymni Homerici, In Dianam"
    "0013:010": "Hymns", # "Hymni Homerici, In Venerem"
    "0013:011": "Hymns", # "Hymni Homerici, In Minervam"
    "0013:012": "Hymns", # "Hymni Homerici, In Junonem"
    "0013:013": "Hymns", # "Hymni Homerici, In Cererem"
    "0013:014": "Hymns", # "Hymni Homerici, In matrem deorum"
    "0013:015": "Hymns", # "Hymni Homerici, In Herculem"
    "0013:016": "Hymns", # "Hymni Homerici, In Aesculapium"
    "0013:017": "Hymns", # "Hymni Homerici, In Dioscuros"
    "0013:018": "Hymns", # "Hymni Homerici, In Mercurium"
    "0013:019": "Hymns", # "Hymni Homerici, In Pana"
    "0013:020": "Hymns", # "Hymni Homerici, In Volcanum"
    "0013:021": "Hymns", # "Hymni Homerici, In Apollinem"
    "0013:022": "Hymns", # "Hymni Homerici, In Neptunum"
    "0013:023": "Hymns", # "Hymni Homerici, In Jovem"
    "0013:024": "Hymns", # "Hymni Homerici, In Vestam"
    "0013:025": "Hymns", # "Hymni Homerici, In Musas et Apollinem"
    "0013:026": "Hymns", # "Hymni Homerici, In Bacchum"
    "0013:027": "Hymns", # "Hymni Homerici, In Dianam"
    "0013:028": "Hymns", # "Hymni Homerici, In Minervam"
    "0013:029": "Hymns", # "Hymni Homerici, In Vestam"
    "0013:030": "Hymns", # "Hymni Homerici, In Tellurem matrem omnium"
    "0013:031": "Hymns", # "Hymni Homerici, In Solem"
    "0013:032": "Hymns", # "Hymni Homerici, In Lunam"
    "0013:033": "Hymns", # "Hymni Homerici, In Dioscuros"
    "0013:034": "Hymns", # "Hymni Homerici, Εἰς ξένους"
    "0020:001": "Theog.",
    "0020:002": "WD",
    "0020:003": "Shield",
    "0020:004": "Fr.", # ??? "Fragmenta"
    "0020:005": "Fr.", # ??? "Testimonia"
    "0020:006": "Fr.", # ??? "Fragmenta astronomica"
    "0020:007": "Fr.", # ??? "Fragmenta"
    "0533:009": "Call.", # "Hecala"
    "0533:015": "Call.", # "In Jovem"
    "0533:016": "Call.", # "In Apollinem"
    "0533:017": "Call.", # "In Dianam"
    "0533:018": "Call.", # "In Delum"
    "0533:020": "Call.", # "In Cererem"
    "0653:001": "Arat",
}

# This is a hack to avoid duplicate erroneous output lines when Diogenes outputs
# overlapping fragments. We keep track of the refs we have already output, and
# ignore an entire fragment whenever we supposedly come across one we've already
# seen.
SEEN = set()

# Represents a TLG location reference.
class Ref(object):
    def __init__(self):
        self.author_id = None
        self.work_id = None
        self.book_id = None
        self.lineno = None
        # lineno_junk is annotations that may come after the line number, such
        # as an asterisk.
        self.lineno_junk = ""

    def __str__(self):
        s = self.author_id + ","+ self.work_id
        if self.book_id is not None:
            s += ":" + self.book_id
        s += ":" + str(self.lineno) + self.lineno_junk
        return s

    def key(self):
        return (self.author_id, self.work_id, self.book_id, self.lineno, self.lineno_junk)

    def author_work(self):
        return self.author_id + ":" + self.work_id

    def location(self):
        book_id = self.book_id
        if self.author_id == "0013":
            # author_id==0013 is a special case for the Homeric Hymns; the work_id
            # indicates which hymn it is. We assign the work_id to the book_id as
            # well, for the sake of having the humn number next to each line number.
            assert book_id is None
            book_id = self.work_id
        if book_id is not None:
            return book_id.lstrip("0") + ":" + str(self.lineno) + self.lineno_junk
        else:
            return str(self.lineno) + self.lineno_junk

# Apply various cleanup functions to the document tree for easier parsing.
def fixup(soup):
    # Unwrap spacer elements. The tags are not closed in the input, so they
    # technically introduce an extra level of hierarchy wherever they are used.
    for elem in soup.find_all("spacer"):
        elem.unwrap()
    # Similarly unwrap <font size="..."></font> elements. The size="-1" ones are
    # not closed (and also seem to have the side effect of capitalizing certain
    # strings within them, e.g. "parse_lat"->"PARSE_LAT" and
    # "jumpTo('tlg...')"→"jumpTo('TLG...')").
    for elem in soup.find_all("font", size=True):
        elem.unwrap()
    # Get rid of empty anchors.
    for elem in soup.find_all("a"):
        if not elem.contents:
            elem.decompose()
    # Get rid of basefont spam.
    for elem in soup.find_all("basefont"):
        elem.decompose()
    # Replace Windows-1252 quotation marks.
    strings = []
    for string in soup.strings:
        strings.append(string)
    for string in strings:
        string.replace_with(str(string).replace(u"\u0093", u"\u201c").replace(u"\u0094", u"\u201d"))

# The results are flat siblings separated by hr elements. Take the pieces
# between each pair of hr elements (including notional ones at the beginning and
# end) and wrap them in section elements.
def split_sections(soup, container):
    section = soup.new_tag("section")
    for elem in list(container.contents):
        elem = elem.extract()
        if elem.name == "hr":
            container.append(section)
            section = soup.new_tag("section")
        else:
            section.append(elem)
    container.append(section)

# Return elements between br elements as a list of span elements. A trailing br
# is ignored, as if br were a '\n' line terminator.
def split_lines(soup, container):
    result = []
    span = soup.new_tag("span")
    for elem in list(container.contents):
        if elem.name == "br":
            result.append(span)
            span = soup.new_tag("span")
        else:
            span.append(elem)
    if span.get_text().strip():
        result.append(span)
    return result

def parse_jumpto(s):
    # Extract a Ref from a Diogenese HTML jumpTo reference, as found in the
    # onlick attribute of links. The links have the form:
    #   <a onclick="jumpTo('tlg,[author],[work]:[book]:[lineno]');">
    # or
    #   <a onclick="jumpTo('tlg,[author],[work]:[lineno]');">
    # for example,
    #   <a onclick="jumpTo('tlg,0001,001:2:394');">
    #   <a onclick="jumpTo('tlg,0020,002:89');">
    # There may be extra junk after the line number:
    #   <a onclick="jumpTo('tlg,0001,001:1:1250*');">
    #
    # "tlg" is normally lowercase, but it can be uppercase, for example in this
    # context:
    # <a onclick="parse_lat('Georg')">Georg.</a>
    # <font size="-1">
    #   <a onclick="PARSE_LAT('I')">I.</a>
    #   <br>
    #   <p>
    #     <a onclick="jumpTo('TLG,0020,004:291:4');">Go to Context</a>
    #   </p>
    #   <hr>
    # </font>
    m = re.match(r'^jumpTo\(\'tlg,(\w+),([\w():*]+)\'\);$', s, flags=re.I)
    assert m is not None, repr(s)

    author_id = m.group(1)
    parts = m.group(2).split(":")
    try:
        work_id, book_id, lineno = parts
    except ValueError:
        work_id, lineno = parts
        book_id = None
    m = re.match(r'^([0-9]+)(.*)$', lineno)
    lineno = int(m.group(1))
    lineno_junk = m.group(2)

    ref = Ref()
    ref.author_id = author_id
    ref.work_id = work_id
    ref.book_id = book_id
    ref.lineno = lineno
    ref.lineno_junk = lineno_junk
    return ref

# Return a 2-tuple of "Incidence of all words as reported by word list" and
# "Passages containing those words reported by Diogenes". Raises ValueError in
# case of error.
def get_counts(elem):
    m = re.search(r'\bIncidence of all words as reported by word list: (\d+)\nPassages containing those words reported by Diogenes: (\d+)\n', elem.get_text())
    if m is None:
        raise ValueError("no summary found")
    return int(m.group(1)), int(m.group(2))

def find_metrical_position(scansion, offset):
    words = []
    word = []
    for c, _, s in scansion:
        if " " in c:
            words.append("".join(word))
            word = []
        elif s == "+" or s == "-":
            word.append(s)
    if word:
        words.append("".join(word))

    pos = 0
    for i in range(0, offset):
        for s in words[i]:
            if s == "+":
                pos += 1
            elif s == "-":
                pos += 0.5
    # Now consume one more syllable. We seek ahead to the first scansion element
    # that is not '', because some words (like "δ'") have no metrical value on
    # their own, but inherit that of the following word.
    i = offset
    while not words[i]:
        i += 1
    if words[i][0] == "+":
        pos += 1
    elif words[i][0] == "-":
        pos += 0.5
    else:
        assert False

    return pos

def unique(a):
    result = []
    for x in a:
        if x not in result:
            result.append(x)
    return result

def fixup_line(line):
    # Some lines start with weird symbols; remove them.
    return re.sub(r'^\s*[>—⸖※]*\s*', "", line)

def process_word(ref, line, form, offset):
    metrical_positions = []
    scansions = hexameter.scan.analyze_line_metrical(fixup_line(line))
    if not scansions:
        # Unable to scan.
        metrical_positions.append("?")
    for scansion in scansions:
        metrical_positions.append("%g" % find_metrical_position(scansion, offset))
    metrical_positions = unique(metrical_positions)

    work_str = ref.author_work()
    print(TLG_MAP.get(work_str, work_str), ref.location(), "/".join(metrical_positions), form, line, sep='\t')

def process_section(soup, section):
    context_link = section.find("p", string="Go to Context", recursive=False)
    if context_link is None:
        raise ValueError("can't find context link: " + str(section))
    context_link = context_link.extract()
    ref = parse_jumpto(context_link.a["onclick"])

    if ref.key() in SEEN:
        print("%s already seen; ignoring this section" % ref, file=sys.stderr)
        return

    # lines will contain also the section header (with the author, title, etc.).
    # It's okay because we only really start paying attention once we see the
    # first highlighted word, which can only appear inside the lines of the
    # actual quotation.
    lines = split_lines(soup, section)

    # Skip lines until we find the first one with a highlighted word; that's the
    # one the ref refers to.
    for i in range(len(lines)):
        line = lines[i]
        if line.find("font", color="red"):
            break
    else:
        raise ValueError("no highlighted words in %s" % ref)

    for i in range(i, len(lines)):
        line = lines[i]
        text = line.get_text().strip()
        # Output all the highlighted words on this line.
        offset = 0
        for elem in line.children:
            if elem.name == "a":
                if elem.find("font", color="red"):
                    process_word(ref, text, elem.get_text(), offset)
                    SEEN.add(ref.key())
                offset += 1
        # There may be further highlighted words on following lines; increment
        # the line number.
        ref.lineno += 1
        ref.lineno_junk = ""

def process(soup):
    fixup(soup)

    main_window = soup.find(id="main_window")
    split_sections(soup, main_window)
    sections = main_window.find_all("section")
    # First section is a header/summary.
    # Penultimate section is a count of incidences.
    # Final section is a footer.
    try:
        incidence_count, passage_count = get_counts(sections[-2])
        sections = sections[1:-2]
    except ValueError:
        # But some files lack the footer, and instead have a final empty
        # section.
        incidence_count, passage_count = None, None
        assert sections[-1].get_text().strip() == ""
        sections = sections[1:-1]

    for section in sections:
        process_section(soup, section)

    # assert incidence_count is None or incidence_count == len(SEEN), "expected %d incidences, found %d" % (incidence_count, len(SEEN))
    assert passage_count is None or passage_count == len(sections), "expected %d passages, found %d" % (passage_count, len(sections))

_, args = getopt.gnu_getopt(sys.argv[1:], "")
if args:
    for filename in args:
        with open(filename) as f:
            process(BeautifulSoup(f, "lxml"))
else:
    process(BeautifulSoup(sys.stdin, "lxml"))
