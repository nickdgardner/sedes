#!/usr/bin/env python3

# Produces a wordâ€”lemma mapping from a CLTK-format JSON file.
#
# Usage:
#   make-lemma-map greek-analyses*.json > lemma-map
#   gzip -9 --rsyncable lemma-map
#
# The input format is JSON as found at
# https://github.com/cltk/greek_lexica_perseus/tree/f370a7e07d0705824b31b1e561ba34cde61b0a45.
# Those JSON files are produced, I believe, by scraping the
# https://www.perseus.tufts.edu/hopper/morph web interface to Morpheus (see the
# scraper.py program in the greek_lexica_perseus repository). The Morpheus
# program is from https://github.com/PerseusDL/morpheus.
#
# The output format is tab-separated, with each line representing all the words
# that map to a single lemma. All words and lemmata are in lower case.
#   lemma\tword1\tword2\tword3...
#
# When there are multiple possible lemmata for a word, the most frequent one is
# used, with ties broken arbitrarily.
#
# This program is similar to
# https://github.com/cltk/greek_lexica_perseus/blob/f370a7e07d0705824b31b1e561ba34cde61b0a45/transform_lemmata.py
# except that it takes input from JSON files instead of text files, and outputs
# a text-based file format instead of a Python dict literal.

import getopt
import json
import sys
import unicodedata

import betacode

def usage(file=sys.stdout):
    print("""\
Usage: {} greek-analyses*.json > lemma-map

greek-analyses*.json are from https://github.com/cltk/greek_lexica_perseus.
""".format(sys.argv[0]), end="", file=file)

opts, args = getopt.gnu_getopt(sys.argv[1:], "h", ["help"])
for o, a in opts:
    if o in ("-h", "--help"):
        usage()
        sys.exit(0)

MAP = {}
for filename in args:
    with open(filename) as f:
        data = json.load(f)
        for word, entries in data.items():
            try:
                word = betacode.decode(word).lower()
            except ValueError as err:
                print("warning: {}: ignoring unparseable beta code {!r}".format(filename, word), file=sys.stderr)
                continue

            if entries is None:
                entries = []
            for entry in entries:
                lemma = unicodedata.normalize("NFD", entry["headword"].lower())
                MAP.setdefault(word, set())
                MAP[word].add(lemma)

LEMMA_COUNTS = {}
for lemmata in MAP.values():
    for lemma in lemmata:
        LEMMA_COUNTS.setdefault(lemma, 0)
        LEMMA_COUNTS[lemma] += 1

OUTPUT_MAP = {}
for word, lemmata in MAP.items():
    if not lemmata:
        continue
    # Choose the most frequent lemma. Break ties with lexicographic ordering.
    lemma = max(lemmata, key = lambda lemma: (LEMMA_COUNTS[lemma], lemma))
    OUTPUT_MAP.setdefault(lemma, [])
    OUTPUT_MAP[lemma].append(word)

for lemma, words in sorted(OUTPUT_MAP.items()):
    assert "\t" not in lemma, lemma
    print(lemma, end="")
    for word in sorted(words):
        assert "\t" not in word, word
        print("\t" + word, end="")
    print()
